{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, learning_curve\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets Used\n",
    "\n",
    "Freddie Mac has created a smaller dataset, which is a random sample of 50,000 loans selected from each full vintage year. Each vintage year has one origination data file and one monthly performance file, containing the same loan-level data fields as those included in the full dataset. We have located the `sample_2016.zip` file from the full dataset package, and used this zip package as our data source for this iteration.\n",
    "\n",
    "The 2016 zip packages has two files: `sample_orig_2016.txt` and `sample_svcg_2016.txt`. The .txt files do not come with headers but instead, we refer to the User Guide (http://www.freddiemac.com/research/pdf/user_guide.pdf) to grab the name of the columns. We then join the two data files together by the loan number. \n",
    "\n",
    "It is expected that as we progressed further, we will be using larger and larger datasets. But for this first iteration, this is what we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    dir = 'D:\\\\Backups\\\\StemData\\\\'\n",
    "    files = ['sample_orig_2016.txt', 'sample_orig_2015.txt', 'sample_orig_2014.txt', 'sample_orig_2013.txt',\n",
    "             'sample_orig_2012.txt', 'sample_orig_2011.txt',\n",
    "             'sample_orig_2010.txt', 'sample_orig_2009.txt', 'sample_orig_2008.txt', 'sample_orig_2007.txt']\n",
    "\n",
    "    files1 = ['sample_svcg_2016.txt', 'sample_svcg_2015.txt', 'sample_svcg_2014.txt', 'sample_svcg_2013.txt',\n",
    "              'sample_svcg_2012.txt', 'sample_svcg_2011.txt',\n",
    "              'sample_svcg_2010.txt', 'sample_svcg_2009.txt', 'sample_svcg_2008.txt', 'sample_svcg_2008.txt']\n",
    "\n",
    "    merged_data = pd.DataFrame()\n",
    "    for i in [0]:\n",
    "        print(files[i])\n",
    "        raw = pd.read_csv(dir + files[i], sep='|', header=None, low_memory=False)\n",
    "        raw.columns = ['credit_score', 'first_pmt_date', 'first_time', 'mat_date', 'msa', 'mi_perc', 'units',\n",
    "                       'occ_status', 'ocltv', 'odti', 'oupb', 'oltv', 'oint_rate', 'channel', 'ppm', 'fixed_rate',\n",
    "                       'state', 'prop_type', 'zip', 'loan_num', 'loan_purpose', 'oterm', 'num_borrowers', 'seller_name',\n",
    "                       'servicer_name', 'exceed_conform']\n",
    "\n",
    "        raw1 = pd.read_csv(dir + files1[i], sep='|', header=None, low_memory=False)\n",
    "        raw1.columns = ['loan_num', 'yearmon', 'curr_upb', 'curr_delinq', 'loan_age', 'remain_months', 'repurchased',\n",
    "                        'modified', 'zero_bal', 'zero_date', 'curr_rate', 'curr_def_upb', 'ddlpi', 'mi_rec',\n",
    "                        'net_proceeds',\n",
    "                        'non_mi_rec', 'exp', 'legal_costs', 'maint_exp', 'tax_insur', 'misc_exp', 'loss', 'mod_exp']\n",
    "\n",
    "        data = pd.merge(raw, raw1, on='loan_num', how='inner')\n",
    "\n",
    "        merged_data = merged_data.append(data)\n",
    "\n",
    "    merged_data.drop(['seller_name', 'servicer_name', 'first_pmt_date', 'mat_date', 'msa', 'net_proceeds'], axis=1, inplace=True)\n",
    "\n",
    "    # all data must have the following: credit_score, ocltv, odti, oltv, oint_rate, curr_upb\n",
    "    # remove any datapoints with missing values from the above features\n",
    "    merged_data.dropna(subset=['credit_score', 'odti', 'oltv', 'oint_rate', 'curr_upb'], how='any', inplace=True)\n",
    "    merged_data.credit_score = pd.to_numeric(data['credit_score'], errors='coerce')\n",
    "    merged_data.yearmon = pd.to_datetime(data['yearmon'], format='%Y%m')\n",
    "    merged_data.fillna(value=0, inplace=True, axis=1)\n",
    "    \n",
    "    merged_data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this into the next func\n",
    "    merged_data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this into the next func\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data: Treatment of Missing Values\n",
    "\n",
    "Key features that are missing are more likely to be the result of reporting errors by the originator or the servicer, or incomplete information provided by the borrower. Similar to the Deep Learning paper we are reading, we have insisted that an observation must have no missing values in any of the following:\n",
    "\n",
    "* FICO score\n",
    "\n",
    "* LTV ratio\n",
    "\n",
    "* Original interest rate\n",
    "\n",
    "* original balance\n",
    "\n",
    "Samples missing one of these variables are removed. \n",
    "\n",
    "After this step, we still have lots of missing values -- a lot of them came from the performance file (such as loan modification costs, legal expenses, etc). Our treatment so far is to treat the missing values as zero, as an missing value of these fields tend to be due to the fact that there hasn't been such an incidence yet.\n",
    "\n",
    "It is clear that we will need to fine-tune our current treatment of missing values. This will be done in the second iteration by leveraging research already done by other STEM interns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Space\n",
    "\n",
    "Here, we also model after the Deep Learning for Mortgage Risk paper. In the paper, the authors have enumerated the possible states (current, 30 days delinquent, etc), and together, with other loan_level features (listed in Table 2 and Table 6 in the paper), formed the feature space for their model.\n",
    "\n",
    "We do similar things here. The following code chunk further process the data: \n",
    "\n",
    "* Get the delinquency status that is associated with the loans and last observed month, and add a data column called `prev_delin`, in contrast with `curr_delinq`\n",
    "\n",
    "* Remove the `curr_delinq` from our features but the feature space still has `prev_delinq` variable\n",
    "\n",
    "* Use `curr_delinq` as our taget\n",
    "\n",
    "* For the categorical variables, we convert them into dummy/indicator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    # data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this out\n",
    "    # data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this out\n",
    "    y = data['curr_delinq']\n",
    "    y = y.apply(lambda x: 1 if x not in (0, 1) else 0)\n",
    "    # data['prev_delinq'] = data.curr_delinq.shift(1) ## needs attention here\n",
    "    # data['prev_delinq'] = data.groupby(level=0)['curr_delinq'].shift(1)\n",
    "    # print(sum(data.prev_delinq.isnull()))\n",
    "    data.fillna(value=0, inplace=True, axis=1)\n",
    "    data.drop(['curr_delinq'], axis=1, inplace=True)\n",
    "    print(y.shape)\n",
    "    ## how many classes are y?\n",
    "    ## remove y from X\n",
    "\n",
    "    X = pd.get_dummies(data)\n",
    "\n",
    "    # X.net_proceeds = X.net_proceeds.apply(lambda x:0 if x == 'C' else x)\n",
    "    # y = label_binarize(y, classes=[0, 1, 2, 3]) ## do we really have to do this?\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    #X[['credit_score', 'mi_perc', 'units', 'ocltv', 'oupb', 'oltv', 'oint_rate', 'zip',\n",
    "    #   'curr_upb', 'loan_age', 'remain_months', 'curr_rate', 'curr_def_upb', 'ddlpi', 'mi_rec',\n",
    "    #   'non_mi_rec', 'exp', 'legal_costs', 'maint_exp', 'tax_insur', 'misc_exp', 'loss', 'mod_exp']] = \\\n",
    "    #    scale(X[['credit_score', 'mi_perc', 'units', 'ocltv', 'oupb', 'oltv', 'oint_rate', 'zip',\n",
    "    #             'curr_upb', 'loan_age', 'remain_months', 'curr_rate', 'curr_def_upb', 'ddlpi', 'mi_rec',\n",
    "    #             'non_mi_rec', 'exp', 'legal_costs', 'maint_exp', 'tax_insur', 'misc_exp', 'loss', 'mod_exp']],\n",
    "    #          with_mean=False)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperparameter Grid Search Across Multiple Models in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=10, n_jobs=-1, verbose=5, scoring=None, refit=True):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': scores.mean(),\n",
    "                'std_score': scores.std()\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = [row(k, gsc.cv_validation_scores, gsc.parameters)\n",
    "                for k in self.keys\n",
    "                for gsc in self.grid_searches[k].grid_scores_]\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        print(df[columns])\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexity():\n",
    "    helper1 = EstimatorSelectionHelper(models, params2)\n",
    "    all_data = get_all_data.get_all_data()\n",
    "    train, target = get_all_data.process_data(all_data)\n",
    "    training_features, test_features, \\\n",
    "    training_target, test_target, = train_test_split(train, target, test_size=0.33, random_state=778)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(training_features, training_target)\n",
    "    helper1.fit(X_train, y_train, scoring='f1', n_jobs=1)\n",
    "    helper1.score_summary(sort_by='min_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'DecisionTree': DecisionTreeClassifier(class_weight='balanced', max_depth=13),\n",
    "    'NeuralNetwork': MLPClassifier(hidden_layer_sizes=(160)),\n",
    "    'GradientBoosting': GradientBoostingClassifier(max_depth=1, n_estimators=50),\n",
    "    'SupportVectorMachine': LinearSVC(class_weight='balanced'),\n",
    "    'KNearestNeighbor': KNeighborsClassifier(n_neighbors=5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params2 = {\n",
    "    'DecisionTree': {'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]},\n",
    "    'NeuralNetwork': {'hidden_layer_sizes': [(160), (160, 112, 112), (160, 112, 112, 112, 112), (160, 112, 112, 112, 112, 112, 112)]},\n",
    "    'GradientBoosting': {'max_depth': [1, 2, 3]},\n",
    "    'SupportVectorMachine': {'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
    "    'KNearestNeighbor': {'n_neighbors': [3,7,11]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analysis2 = complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='roc_auc')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = get_all_data()\n",
    "train, target = process_data(all_data)\n",
    "for model in models:\n",
    "    title = 'Learning Curves: ' + model\n",
    "    cv = ShuffleSplit(n_splits=10, test_size=0.2)\n",
    "    print(title)\n",
    "    plot_learning_curve(models[model], title, train, target, cv=cv, n_jobs=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
