{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# First iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get data and build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from joblib import Parallel, delayed\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets Used\n",
    "\n",
    "Freddie Mac has created a smaller dataset, which is a random sample of 50,000 loans selected from each full vintage year. Each vintage year has one origination data file and one monthly performance file, containing the same loan-level data fields as those included in the full dataset. We have located the `sample_2016.zip` file from the full dataset package, and used this zip package as our data source for this iteration.\n",
    "\n",
    "The 2016 zip packages has two files: `sample_orig_2016.txt` and `sample_svcg_2016.txt`. The .txt files do not come with headers but instead, we refer to the User Guide (http://www.freddiemac.com/research/pdf/user_guide.pdf) to grab the name of the columns. We then join the two data files together by the loan number. \n",
    "\n",
    "It is expected that as we progressed further, we will be using larger and larger datasets. But for this first iteration, this is what we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    dir = 'D:\\\\Backups\\\\StemData\\\\'\n",
    "    files = ['sample_orig_2016.txt', 'sample_orig_2015.txt', 'sample_orig_2014.txt', 'sample_orig_2013.txt',\n",
    "             'sample_orig_2012.txt', 'sample_orig_2011.txt',\n",
    "             'sample_orig_2010.txt', 'sample_orig_2009.txt', 'sample_orig_2008.txt', 'sample_orig_2007.txt']\n",
    "\n",
    "    files1 = ['sample_svcg_2016.txt', 'sample_svcg_2015.txt', 'sample_svcg_2014.txt', 'sample_svcg_2013.txt',\n",
    "              'sample_svcg_2012.txt', 'sample_svcg_2011.txt',\n",
    "              'sample_svcg_2010.txt', 'sample_svcg_2009.txt', 'sample_svcg_2008.txt', 'sample_svcg_2008.txt']\n",
    "\n",
    "    merged_data = pd.DataFrame()\n",
    "    for i in [0]:\n",
    "        print(files[i])\n",
    "        raw = pd.read_csv(dir + files[i], sep='|', header=None, low_memory=False)\n",
    "        raw.columns = ['credit_score', 'first_pmt_date', 'first_time', 'mat_date', 'msa', 'mi_perc', 'units',\n",
    "                       'occ_status', 'ocltv', 'odti', 'oupb', 'oltv', 'oint_rate', 'channel', 'ppm', 'fixed_rate',\n",
    "                       'state', 'prop_type', 'zip', 'loan_num', 'loan_purpose', 'oterm', 'num_borrowers', 'seller_name',\n",
    "                       'servicer_name', 'exceed_conform']\n",
    "\n",
    "        raw1 = pd.read_csv(dir + files1[i], sep='|', header=None, low_memory=False)\n",
    "        raw1.columns = ['loan_num', 'yearmon', 'curr_upb', 'curr_delinq', 'loan_age', 'remain_months', 'repurchased',\n",
    "                        'modified', 'zero_bal', 'zero_date', 'curr_rate', 'curr_def_upb', 'ddlpi', 'mi_rec',\n",
    "                        'net_proceeds',\n",
    "                        'non_mi_rec', 'exp', 'legal_costs', 'maint_exp', 'tax_insur', 'misc_exp', 'loss', 'mod_exp']\n",
    "\n",
    "        data = pd.merge(raw, raw1, on='loan_num', how='inner')\n",
    "\n",
    "        merged_data = merged_data.append(data)\n",
    "\n",
    "    merged_data.drop(['seller_name', 'servicer_name', 'first_pmt_date', 'mat_date', 'msa', 'net_proceeds'], axis=1, inplace=True)\n",
    "\n",
    "    # all data must have the following: credit_score, ocltv, odti, oltv, oint_rate, curr_upb\n",
    "    # remove any datapoints with missing values from the above features\n",
    "    merged_data.dropna(subset=['credit_score', 'odti', 'oltv', 'oint_rate', 'curr_upb'], how='any', inplace=True)\n",
    "    merged_data.credit_score = pd.to_numeric(data['credit_score'], errors='coerce')\n",
    "    merged_data.yearmon = pd.to_datetime(data['yearmon'], format='%Y%m')\n",
    "    merged_data.fillna(value=0, inplace=True, axis=1)\n",
    "    \n",
    "    merged_data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this into the next func\n",
    "    merged_data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this into the next func\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment of Missing Values (So Far)\n",
    "\n",
    "Key features that are missing are more likely to be the result of reporting errors by the originator or the servicer, or incomplete information provided by the borrower. Similar to the Deep Learning paper we are reading, we have insisted that an observation must have no missing values in any of the following:\n",
    "\n",
    "* FICO score\n",
    "\n",
    "* LTV ratio\n",
    "\n",
    "* Original interest rate\n",
    "\n",
    "* original balance\n",
    "\n",
    "Samples missing one of these variables are removed. \n",
    "\n",
    "After this step, we still have lots of missing values -- a lot of them came from the performance file (such as loan modification costs, legal expenses, etc). Our treatment so far is to treat the missing values as zero, as an missing value of these fields tend to be due to the fact that there hasn't been such an incidence yet.\n",
    "\n",
    "It is clear that we will need to fine-tune our current treatment of missing values. This will be done in the second iteration by leveraging research already done by other STEM interns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Space\n",
    "\n",
    "Here, we also model after the Deep Learning for Mortgage Risk paper. In the paper, the authors have enumerated the possible states (current, 30 days delinquent, etc), and together, with other loan_level features (listed in Table 2 and Table 6 in the paper), formed the feature space for their model.\n",
    "\n",
    "We do similar things here. The following code chunk further process the data: \n",
    "\n",
    "* Get the delinquency status that is associated with the loans and last observed month, and add a data column called `prev_delin`, in contrast with `curr_delinq`\n",
    "\n",
    "* Remove the `curr_delinq` from our features but the feature space still has `prev_delinq` variable\n",
    "\n",
    "* Use `curr_delinq` as our taget\n",
    "\n",
    "* For the categorical variables, we convert them into dummy/indicator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    #data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this out\n",
    "    #data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this out\n",
    "    y = data['curr_delinq']\n",
    "    y = y.apply(lambda x:1 if x not in (0, 1) else 0)\n",
    "    #data['prev_delinq'] = data.curr_delinq.shift(1) ## needs attention here\n",
    "    #data['prev_delinq'] = data.groupby(level=0)['curr_delinq'].shift(1)\n",
    "    #print(sum(data.prev_delinq.isnull()))\n",
    "    data.fillna(value=0, inplace=True, axis=1)\n",
    "    data.drop(['curr_delinq'], axis=1, inplace=True)\n",
    "    print(y.shape)\n",
    "    ## how many classes are y?\n",
    "    ## remove y from X\n",
    "    \n",
    "    X = pd.get_dummies(data)\n",
    "    #X.net_proceeds = X.net_proceeds.apply(lambda x:0 if x == 'C' else x)\n",
    "    #y = label_binarize(y, classes=[0, 1, 2, 3]) ## do we really have to do this?\n",
    "    X[['credit_score','mi_perc','units','ocltv', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']] = \\\n",
    "    scale(X[['credit_score','mi_perc','units','ocltv', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']], with_mean=False)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting plotting utility ready\n",
    "\n",
    "We define the function to plot confusion matrix beow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dict(dicts):\n",
    "    \"\"\"dicts: a list of dicts\"\"\"\n",
    "    super_dict = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():  # d.items() in Python 3+\n",
    "            super_dict.setdefault(k, []).append(v)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(super_dict)\n",
    "    df.plot()\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with Pruning\n",
    "\n",
    "Decision Trees. For the decision tree, you should implement or steal a decision tree algorithm (and by \"implement or steal\" I mean \"steal\"). Be sure to use some form of pruning. You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexity_dt(X, y):\n",
    "    #X_train, y_train, X_test, y_test = train_test_split(X,y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=778)\n",
    "    #smote = SMOTE(ratio=1)\n",
    "    #X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    print('Start Search')\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "    pipe = Pipeline([('dt', dt)])\n",
    "    param_grid = {'dt__max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=6, cv=5, scoring='neg_log_loss', verbose=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print('clf', clf)\n",
    "    print('best_score', grid_search.best_score_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    check_pred = clf.predict(X_train)\n",
    "    target_names = ['Not delinq', 'Delinq']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    return clf, clf.predict(X_train), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf, score, mat = complexity_dt(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.itemfreq(score)\n",
    "scipy.stats.itemfreq(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_size_dt(train, target):\n",
    "    d = {'train': [], 'cv set': [], 'test': []}\n",
    "    print('here')\n",
    "    training_features, test_features, \\\n",
    "    training_target, test_target, = train_test_split(train, target, test_size=0.33, random_state=778)\n",
    "    for size in np.arange(0.1, 1, 0.1):\n",
    "        print('size', size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(training_features, training_target, train_size=size)\n",
    "        print('start')\n",
    "        start_time = time.time()\n",
    "        dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "        dt.fit(X_train, y_train)\n",
    "        print('Decision Tree took', time.time() - start_time, 'to run')\n",
    "        print('process')\n",
    "        d['train'].append(f1_score(y_train, dt.predict(X_train), average='weighted'))\n",
    "        d['cv set'].append(f1_score(y_val, dt.predict(X_val), average='weighted'))\n",
    "        d['test'].append(f1_score(test_target, dt.predict(test_features), average='weighted'))\n",
    "        print('end')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to use parallel processing in Windows: must wrap this in the main function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = get_all_data()\n",
    "train, target = process_data(all_data)\n",
    "df = pd.DataFrame.from_dict(train_size_dt(train=train, target=target))\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Network Model: First Iteration\n",
    "\n",
    "\n",
    "We had used the grid search approach to find the the best number of hidden layers (out of 1, 3, 5, and 7). For each of these options, we started out with the full set of features, then reduce it to 70% of that for each subsequent hidden layers.\n",
    "\n",
    "The authors' deep learning give them a probability transition matrix. \n",
    "\n",
    "Our model below gives us a probability matrix for each observation data. This is slightly different.\n",
    "\n",
    "However, with a bit more work, we can convert our probability matrix produced from our model into the probability transition matrix, so that it not only predicts for us, when a new data comes in, what is the most likely delinquent status of a new loan, but also tell us what is the probability that a loan of a particular delinquency status will transition into a different status type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexity_ann(X, y):\n",
    "    # X_train, y_train, X_test, y_test = train_test_split(X,y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=778)\n",
    "    # smote = SMOTE(ratio=1)\n",
    "    # X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    print('Start Search')\n",
    "    mlp = MLPClassifier(verbose=True)\n",
    "    pipe = Pipeline([('mlp', mlp)])\n",
    "    param_grid = {\n",
    "        'mlp__hidden_layer_sizes': [(160), (160, 112, 112), (160, 112, 112, 112, 112), (160, 112, 112, 112, 112, 112, 112)]}\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=6, cv=10, scoring='neg_log_loss', verbose=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print('clf', clf)\n",
    "    print('best_score', grid_search.best_score_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    check_pred = clf.predict(X_train)\n",
    "    target_names = ['Not delinq', 'Delinq']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=target_names,\n",
    "                                  title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    return clf, clf.predict(X_train), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf, score, mat = complexity_ann(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_size_ann(train, target):\n",
    "    d = {'train': [], 'cv set': [], 'test': []}\n",
    "    print('here')\n",
    "    training_features, test_features, \\\n",
    "    training_target, test_target, = train_test_split(train, target, test_size=0.33, random_state=778)\n",
    "    for size in np.arange(0.1, 1, 0.1):\n",
    "        print('size', size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(training_features, training_target, train_size=size)\n",
    "        print('start')\n",
    "        start_time = time.time()\n",
    "        clf = MLPClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Neural Network Tree took', time.time() - start_time, 'to run')\n",
    "        d['train'].append(f1_score(y_train, clf.predict(X_train), average='weighted'))\n",
    "        d['cv set'].append(f1_score(y_val, clf.predict(X_val), average='weighted'))\n",
    "        d['test'].append(f1_score(test_target, clf.predict(test_features), average='weighted'))\n",
    "        print('end')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = get_all_data()\n",
    "train, target = process_data(all_data)\n",
    "df = pd.DataFrame.from_dict(train_size_ann(train=train, target=target))\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexity_boost(X, y):\n",
    "    #X_train, y_train, X_test, y_test = train_test_split(X,y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=778)\n",
    "    smote = SMOTE(ratio=1)\n",
    "    X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    print('Start Decision Tree Search')\n",
    "    boost = GradientBoostingClassifier(n_estimators=100)\n",
    "    pipe = Pipeline([('smote', smote), ('boost', boost)])\n",
    "    param_grid = {'boost__max_depth': [1, 2, 3]}\n",
    "    #sss = StratifiedShuffleSplit(n_splits=500, test_size=0.2)  ## no need for this given 50000 random sample\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=6, cv=10, scoring='neg_log_loss',verbose=5)\n",
    "    grid_search.fit(X_train_res, y_train_res)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print('clf', clf)\n",
    "    print('best_score', grid_search.best_score_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    check_pred = clf.predict(X_train)\n",
    "    target_names = ['Not delinq', 'Delinq']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "    plt.show()    \n",
    "    return clf, clf.predict(X_train_res), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf, score, mat = complexity_boost(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_size_boost(train, target, size=0):\n",
    "    d = {'train': [], 'cv set': [], 'test': []}\n",
    "    training_features, test_features, \\\n",
    "    training_target, test_target, = train_test_split(train, target, test_size=0.33, random_state=778)\n",
    "    for size in np.arange(0.1, 1, 0.1):\n",
    "        print('size', size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(training_features, training_target, train_size=size)\n",
    "        print('start')\n",
    "        start_time = time.time()\n",
    "        clf = GradientBoostingClassifier(n_estimators=1000)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('Decision Tree took', time.time() - start_time, 'to run')\n",
    "        d['train'].append(f1_score(y_train, clf.predict(X_train), average='weighted'))\n",
    "        d['cv set'].append(f1_score(y_val, clf.predict(X_val), average='weighted'))\n",
    "        d['test'].append(f1_score(test_target, clf.predict(test_features), average='weighted'))\n",
    "        print('end')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = get_all_data()\n",
    "train, target = process_data(all_data)\n",
    "df = pd.DataFrame.from_dict(train_size_boost(train=train, target=target))\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexity_svc(X, y):\n",
    "    #X_train, y_train, X_test, y_test = train_test_split(X,y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=778)\n",
    "    #smote = SMOTE(ratio=1)\n",
    "    #X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    print('Start Search')\n",
    "    svm= SVC(class_weight='balanced', probability=True)\n",
    "    pipe = Pipeline([('svm', svm)])\n",
    "    param_grid = {\n",
    "        'svm__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=6, cv=3, scoring='neg_log_loss', verbose=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print('clf', clf)\n",
    "    print('best_score', grid_search.best_score_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    check_pred = clf.predict(X_train)\n",
    "    target_names = ['Not delinq', 'Delinq']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    return clf, clf.predict(X_train_res), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf, score, mat = complexity_svc(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_size_svc(train=None, target=None, size=0):\n",
    "    d = {'train': [], 'cv set': [], 'test': []}\n",
    "    training_features, test_features, \\\n",
    "    training_target, test_target, = train_test_split(train, target, test_size=0.33, random_state=778)\n",
    "    for size in np.arange(0.1, 1, 0.1):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(training_features, training_target, train_size=size)\n",
    "        print('size', size)\n",
    "        print('start')\n",
    "        clf = SVC(verbose=True)\n",
    "        clf.fit(X_train, y_train)\n",
    "        d['train'].append(f1_score(y_train, clf.predict(X_train), average='weighted'))\n",
    "        d['cv set'].append(f1_score(y_val, clf.predict(X_val), average='weighted'))\n",
    "        d['test'].append(f1_score(test_target, clf.predict(test_features), average='weighted'))\n",
    "        print('end')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = get_all_data()\n",
    "train, target = process_data(all_data)\n",
    "df = df = pd.DataFrame.from_dict(train_size_svc(train=train, target=target))\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_size_knn(train=None, target=None, size=0):\n",
    "    d = {'train': [], 'cv set': [], 'test': []}\n",
    "    training_features, test_features, \\\n",
    "    training_target, test_target, = train_test_split(train, target, test_size=0.33, random_state=778)\n",
    "    for size in np.arange(0.1, 1, 0.1):\n",
    "        print('size', size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(training_features, training_target, train_size=size)\n",
    "        print('start')\n",
    "        start_time = time.time()\n",
    "        clf = KNeighborsClassifier(n_neighbors=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print('KNN took', time.time() - start_time, 'to run')\n",
    "        d['train'].append(f1_score(y_train, clf.predict(X_train), average='weighted'))\n",
    "        d['cv set'].append(f1_score(y_val, clf.predict(X_val), average='weighted'))\n",
    "        d['test'].append(f1_score(test_target, clf.predict(test_features), average='weighted'))\n",
    "        print('end')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_orig_2016.txt\n",
      "(203642,)\n",
      "size 0.1\n",
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN took 0.3795289993286133 to run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n",
      "size 0.2\n",
      "start\n",
      "KNN took 1.6685278415679932 to run\n",
      "end\n",
      "size 0.3\n",
      "start\n",
      "KNN took 6.546721458435059 to run\n",
      "end\n",
      "size 0.4\n",
      "start\n",
      "KNN took 12.989205360412598 to run\n",
      "end\n",
      "size 0.5\n",
      "start\n",
      "KNN took 23.61362075805664 to run\n",
      "end\n",
      "size 0.6\n",
      "start\n",
      "KNN took 31.310216426849365 to run\n",
      "end\n",
      "size 0.7\n",
      "start\n",
      "KNN took 49.789954662323 to run\n",
      "end\n",
      "size 0.8\n",
      "start\n",
      "KNN took 69.1616063117981 to run\n",
      "end\n",
      "size 0.9\n",
      "start\n",
      "KNN took 89.00938153266907 to run\n",
      "end\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-903290d61dc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_size_knn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmerge_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-a4bb012316d6>\u001b[0m in \u001b[0;36mmerge_dict\u001b[1;34m(dicts)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msuper_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# d.items() in Python 3+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0msuper_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "all_data = get_all_data()\n",
    "train, target = process_data(all_data)\n",
    "df = pd.DataFrame.from_dict(train_size_knn(train=train, target=target))\n",
    "merge_dict(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notes \n",
    "\n",
    "1. We had 519 variables and the authors had 294. But to be sure, we don't have a greater number of features compared to the authors. I think this is just an artifact of our different implementations as the authors do have more data than us.\n",
    "\n",
    "2. ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output as we had done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
