{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# First iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get data and build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.tree as tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from joblib import Parallel, delayed\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets Used\n",
    "\n",
    "Freddie Mac has created a smaller dataset, which is a random sample of 50,000 loans selected from each full vintage year. Each vintage year has one origination data file and one monthly performance file, containing the same loan-level data fields as those included in the full dataset. We have located the `sample_2016.zip` file from the full dataset package, and used this zip package as our data source for this iteration.\n",
    "\n",
    "The 2016 zip packages has two files: `sample_orig_2016.txt` and `sample_svcg_2016.txt`. The .txt files do not come with headers but instead, we refer to the User Guide (http://www.freddiemac.com/research/pdf/user_guide.pdf) to grab the name of the columns. We then join the two data files together by the loan number. \n",
    "\n",
    "It is expected that as we progressed further, we will be using larger and larger datasets. But for this first iteration, this is what we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    dir = 'D:\\\\Backups\\\\StemData\\\\'\n",
    "    file = 'sample_orig_2015.txt'\n",
    "    file1 = 'sample_svcg_2015.txt'\n",
    "\n",
    "    raw = pd.read_csv(dir+file, sep='|', header=None)\n",
    "    raw.columns = ['credit_score', 'first_pmt_date', 'first_time', 'mat_date', 'msa', 'mi_perc', 'units',\n",
    "                    'occ_status', 'ocltv', 'odti', 'oupb', 'oltv', 'oint_rate', 'channel', 'ppm', 'fixed_rate',\n",
    "                    'state', 'prop_type','zip','loan_num', 'loan_purpose','oterm','num_borrowers', 'seller_name',\n",
    "                    'servicer_name','exceed_conform']\n",
    "\n",
    "    raw1 = pd.read_csv(dir+file1, sep='|', header=None)\n",
    "    raw1.columns = ['loan_num', 'yearmon', 'curr_upb','curr_delinq','loan_age','remain_months', 'repurchased',\n",
    "                     'modified', 'zero_bal','zero_date','curr_rate','curr_def_upb', 'ddlpi','mi_rec','net_proceeds',\n",
    "                     'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']\n",
    "\n",
    "    data = pd.merge(raw, raw1, on='loan_num', how='inner')\n",
    "\n",
    "    data.drop(['seller_name', 'servicer_name', 'first_pmt_date', 'mat_date', 'msa'], axis=1, inplace=True)\n",
    "    # all data must have the following: credit_score, ocltv, odti, oltv, oint_rate, curr_upb\n",
    "    # remove any datapoints with missing values from the above features\n",
    "    data.dropna(subset=['credit_score', 'odti', 'oltv', 'oint_rate', 'curr_upb'], how='any',inplace=True)\n",
    "    data.credit_score = pd.to_numeric(data['credit_score'], errors='coerce')\n",
    "    data.yearmon = pd.to_datetime(data['yearmon'], format='%Y%m')\n",
    "    data.fillna(value=0, inplace=True, axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    dir = 'D:\\\\Backups\\\\StemData\\\\'\n",
    "    files = ['sample_orig_2016.txt', 'sample_orig_2015.txt', 'sample_orig_2014.txt', 'sample_orig_2013.txt',\n",
    "             'sample_orig_2012.txt', 'sample_orig_2011.txt',\n",
    "             'sample_orig_2010.txt', 'sample_orig_2009.txt', 'sample_orig_2008.txt', 'sample_orig_2007.txt']\n",
    "\n",
    "    files1 = ['sample_svcg_2016.txt', 'sample_svcg_2015.txt', 'sample_svcg_2014.txt', 'sample_svcg_2013.txt',\n",
    "              'sample_svcg_2012.txt', 'sample_svcg_2011.txt',\n",
    "              'sample_svcg_2010.txt', 'sample_svcg_2009.txt', 'sample_svcg_2008.txt', 'sample_svcg_2008.txt']\n",
    "\n",
    "    merged_data = pd.DataFrame()\n",
    "    for i in [1]:\n",
    "        print(files[i])\n",
    "        raw = pd.read_csv(dir + files[i], sep='|', header=None, low_memory=False)\n",
    "        raw.columns = ['credit_score', 'first_pmt_date', 'first_time', 'mat_date', 'msa', 'mi_perc', 'units',\n",
    "                       'occ_status', 'ocltv', 'odti', 'oupb', 'oltv', 'oint_rate', 'channel', 'ppm', 'fixed_rate',\n",
    "                       'state', 'prop_type', 'zip', 'loan_num', 'loan_purpose', 'oterm', 'num_borrowers', 'seller_name',\n",
    "                       'servicer_name', 'exceed_conform']\n",
    "\n",
    "        raw1 = pd.read_csv(dir + files1[i], sep='|', header=None, low_memory=False)\n",
    "        raw1.columns = ['loan_num', 'yearmon', 'curr_upb', 'curr_delinq', 'loan_age', 'remain_months', 'repurchased',\n",
    "                        'modified', 'zero_bal', 'zero_date', 'curr_rate', 'curr_def_upb', 'ddlpi', 'mi_rec',\n",
    "                        'net_proceeds',\n",
    "                        'non_mi_rec', 'exp', 'legal_costs', 'maint_exp', 'tax_insur', 'misc_exp', 'loss', 'mod_exp']\n",
    "\n",
    "        data = pd.merge(raw, raw1, on='loan_num', how='inner')\n",
    "\n",
    "        merged_data = merged_data.append(data)\n",
    "\n",
    "    merged_data.drop(['seller_name', 'servicer_name', 'first_pmt_date', 'mat_date', 'msa', 'net_proceeds'], axis=1, inplace=True)\n",
    "\n",
    "    # all data must have the following: credit_score, ocltv, odti, oltv, oint_rate, curr_upb\n",
    "    # remove any datapoints with missing values from the above features\n",
    "    merged_data.dropna(subset=['credit_score', 'odti', 'oltv', 'oint_rate', 'curr_upb'], how='any', inplace=True)\n",
    "    merged_data.credit_score = pd.to_numeric(data['credit_score'], errors='coerce')\n",
    "    merged_data.yearmon = pd.to_datetime(data['yearmon'], format='%Y%m')\n",
    "    merged_data.fillna(value=0, inplace=True, axis=1)\n",
    "    \n",
    "    merged_data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this into the next func\n",
    "    merged_data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this into the next func\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment of Missing Values (So Far)\n",
    "\n",
    "Key features that are missing are more likely to be the result of reporting errors by the originator or the servicer, or incomplete information provided by the borrower. Similar to the Deep Learning paper we are reading, we have insisted that an observation must have no missing values in any of the following:\n",
    "\n",
    "* FICO score\n",
    "\n",
    "* LTV ratio\n",
    "\n",
    "* Original interest rate\n",
    "\n",
    "* original balance\n",
    "\n",
    "Samples missing one of these variables are removed. \n",
    "\n",
    "After this step, we still have lots of missing values -- a lot of them came from the performance file (such as loan modification costs, legal expenses, etc). Our treatment so far is to treat the missing values as zero, as an missing value of these fields tend to be due to the fact that there hasn't been such an incidence yet.\n",
    "\n",
    "It is clear that we will need to fine-tune our current treatment of missing values. This will be done in the second iteration by leveraging research already done by other STEM interns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = get_data()\n",
    "(raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_orig_2015.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata = get_all_data()\n",
    "len(alldata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alldata.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this into the next func\n",
    "#alldata.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this into the next func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldata.credit_score.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Space\n",
    "\n",
    "Here, we also model after the Deep Learning for Mortgage Risk paper. In the paper, the authors have enumerated the possible states (current, 30 days delinquent, etc), and together, with other loan_level features (listed in Table 2 and Table 6 in the paper), formed the feature space for their model.\n",
    "\n",
    "We do similar things here. The following code chunk further process the data: \n",
    "\n",
    "* Get the delinquency status that is associated with the loans and last observed month, and add a data column called `prev_delin`, in contrast with `curr_delinq`\n",
    "\n",
    "* Remove the `curr_delinq` from our features but the feature space still has `prev_delinq` variable\n",
    "\n",
    "* Use `curr_delinq` as our taget\n",
    "\n",
    "* For the categorical variables, we convert them into dummy/indicator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    #data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this out\n",
    "    #data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this out\n",
    "    y = data['curr_delinq']\n",
    "    y = y.apply(lambda x:1 if x not in (0, 1) else 0)\n",
    "    #data['prev_delinq'] = data.curr_delinq.shift(1) ## needs attention here\n",
    "    #data['prev_delinq'] = data.groupby(level=0)['curr_delinq'].shift(1)\n",
    "    #print(sum(data.prev_delinq.isnull()))\n",
    "    data.fillna(value=0, inplace=True, axis=1)\n",
    "    data.drop(['curr_delinq'], axis=1, inplace=True)\n",
    "    print(y.shape)\n",
    "    ## how many classes are y?\n",
    "    ## remove y from X\n",
    "    \n",
    "    X = pd.get_dummies(data)\n",
    "    #X.net_proceeds = X.net_proceeds.apply(lambda x:0 if x == 'C' else x)\n",
    "    #y = label_binarize(y, classes=[0, 1, 2, 3]) ## do we really have to do this?\n",
    "    X[['credit_score','mi_perc','units','ocltv', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']] = \\\n",
    "    scale(X[['credit_score','mi_perc','units','ocltv', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']], with_mean=False)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data_rev(data):\n",
    "    #data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this out\n",
    "    #data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this out\n",
    "    y = data['curr_delinq']\n",
    "    #data['prev_delinq'] = data.curr_delinq.shift(1) ## needs attention here\n",
    "    data['prev_delinq'] = data.groupby(level=0)['curr_delinq'].shift(1)\n",
    "    print(sum(data.prev_delinq.isnull()))\n",
    "    data.fillna(value=0, inplace=True, axis=1)\n",
    "    data.drop(['curr_delinq'], axis=1, inplace=True)\n",
    "    print(y.shape)\n",
    "    ## how many classes are y?\n",
    "    ## remove y from X\n",
    "    X = pd.get_dummies(data, columns=['first_time', 'occ_status', 'channel', 'ppm', 'fixed_rate',\n",
    "                                  'state', 'prop_type', 'loan_purpose', 'exceed_conform', 'repurchased'])\n",
    "    #y = label_binarize(y, classes=[0, 1, 2, 3]) ## do we really have to do this?\n",
    "    X[['credit_score','mi_perc','units','ocltv', 'odti', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']] = \\\n",
    "    scale(X[['credit_score','mi_perc','units','ocltv', 'odti', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']], with_mean=False)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(833264,)\n"
     ]
    }
   ],
   "source": [
    "train, target = process_data(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alldata.curr_delinq.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting plotting utility ready\n",
    "\n",
    "We define the function to plot confusion matrix beow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with Pruning\n",
    "\n",
    "Decision Trees. For the decision tree, you should implement or steal a decision tree algorithm (and by \"implement or steal\" I mean \"steal\"). Be sure to use some form of pruning. You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw.prev_delinq.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexity_dt(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=778)\n",
    "    smote = SMOTE(ratio=1)\n",
    "    X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    print('Start Decision Tree Search')\n",
    "    clf = tree.DecisionTreeClassifier(criterion='gini', class_weight='balanced')\n",
    "    pipe = Pipeline([('smote', smote), ('dt', clf)])\n",
    "    param_grid = {'dt__max_depth': [2, 3, 4, 5, 6, 7, 8]}\n",
    "    #sss = StratifiedShuffleSplit(n_splits=500, test_size=0.2)  ## no need for this given 50000 random sample\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=6, cv=10, scoring='neg_log_loss',verbose=5)\n",
    "    grid_search.fit(X_train_res, y_train_res)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print('clf', clf)\n",
    "    print('best_score', grid_search.best_score_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    check_pred = clf.predict(X_train)\n",
    "    target_names = ['Not delinq', 'Delinq']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "    plt.show()    \n",
    "    return clf, clf.predict(X_train_res), y_pred\n",
    "\n",
    "\n",
    "dt, predict_dt, result_dt = complexity_dt(train, target)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.itemfreq(predict_dt)\n",
    "scipy.stats.itemfreq(result_dt)\n",
    "\n",
    "\n",
    "#import graphviz \n",
    "#dot_data = tree.export_graphviz(dt, out_file=None) \n",
    "#graph = graphviz.Source(dot_data) \n",
    "#graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traing_size_dt(X, y):\n",
    "    d = {'train':[], 'cv set':[], 'test':[]}\n",
    "    training_features, test_features, \\\n",
    "    training_target, test_target, = train_test_split(X, y, test_size = 0.2, random_state=12)\n",
    "    for size in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(training_features, training_target, train_size=size)\n",
    "        smote = SMOTE(ratio=1)\n",
    "        X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "         \n",
    "        clf = tree.DecisionTreeClassifier(criterion='gini', class_weight='balanced', max_depth=8)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        from sklearn.metrics import f1_score\n",
    "        \n",
    "        d['train'].append(f1_score(y_train_res, clf.predict(X_train_res), average='weighted'))\n",
    "        d['cv set'].append(f1_score(y_val, clf.predict(X_val), average='weighted'))\n",
    "        d['test'].append(f1_score(test_target, clf.predict(test_features), average='weighted'))\n",
    "        \n",
    "    return d\n",
    "\n",
    "dt = traing_size_dt(train, target)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_train_res, y, train_size=size, random_state=778)\n",
    "        clf = tree.DecisionTreeClassifier(criterion='gini', class_weight='balanced', max_depth=8)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        check_pred = clf.predict(X_train_res)\n",
    "        from collections import defaultdict\n",
    "        from sklearn.metrics import f1_score\n",
    "        \n",
    "        d = defaultdict(list)\n",
    "        d['size '+str((size*10))].append(precision_score(y_train_sub, check_pred, average='weighted')) \n",
    "        d['size '+str((size*10))].append(precision_score(y_test, y_pred, average='weighted')) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.itemfreq(check)\n",
    "scipy.stats.itemfreq(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e in dt:\n",
    "    print(e)\n",
    "    print(dt[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Network Model: First Iteration\n",
    "\n",
    "\n",
    "We had used the grid search approach to find the the best number of hidden layers (out of 1, 3, 5, and 7). For each of these options, we started out with the full set of features, then reduce it to 70% of that for each subsequent hidden layers.\n",
    "\n",
    "The authors' deep learning give them a probability transition matrix. \n",
    "\n",
    "Our model below gives us a probability matrix for each observation data. This is slightly different.\n",
    "\n",
    "However, with a bit more work, we can convert our probability matrix produced from our model into the probability transition matrix, so that it not only predicts for us, when a new data comes in, what is the most likely delinquent status of a new loan, but also tell us what is the probability that a loan of a particular delinquency status will transition into a different status type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridSearch_nn(X, y):\n",
    "    #X_train, y_train, X_test, y_test = train_test_split(X,y)\n",
    "    mlp = MLPClassifier(solver='adam', alpha=1e-5, shuffle=True, learning_rate='invscaling',\n",
    "         verbose=True)\n",
    "    parameters = {'hidden_layer_sizes':[(160), (160, 112, 112), (160, 112, 112, 112, 112), (160, 112, 112, 112, 112, 112, 112)]}\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)  ## no need for this given 50000 random sample\n",
    "    gs = GridSearchCV(estimator=mlp, param_grid=parameters, n_jobs=6, cv=sss, scoring='neg_log_loss',verbose=3)\n",
    "    gs.fit(X, y)\n",
    "    clf = gs.best_estimator_\n",
    "    print(clf)\n",
    "    print(gs.best_score_)\n",
    "    mat = clf.predict_proba(X)\n",
    "    print(mat)\n",
    "    \n",
    "    return clf, gs.best_score_, mat\n",
    "\n",
    "\n",
    "\n",
    "clf, score, mat = gridSearch_nn(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:75: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Decision Tree Search\n",
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "def complexity_boost(X, y):\n",
    "    #X_train, y_train, X_test, y_test = train_test_split(X,y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=778)\n",
    "    smote = SMOTE(ratio=1)\n",
    "    X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    print('Start Decision Tree Search')\n",
    "    boost = GradientBoostingClassifier(n_estimators=50)\n",
    "    pipe = Pipeline([('smote', smote), ('xgb', boost)])\n",
    "    param_grid = {'xgb__max_depth': [1, 2, 3]}\n",
    "    #sss = StratifiedShuffleSplit(n_splits=500, test_size=0.2)  ## no need for this given 50000 random sample\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=6, cv=10, scoring='neg_log_loss',verbose=5)\n",
    "    grid_search.fit(X_train_res, y_train_res)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print('clf', clf)\n",
    "    print('best_score', grid_search.best_score_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    check_pred = clf.predict(X_train)\n",
    "    target_names = ['Not delinq', 'Delinq']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "    plt.show()    \n",
    "    return clf, clf.predict(X_train_res), y_pred\n",
    "\n",
    "\n",
    "\n",
    "clf, score, mat = complexity_boost(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notes \n",
    "\n",
    "1. We had 519 variables and the authors had 294. But to be sure, we don't have a greater number of features compared to the authors. I think this is just an artifact of our different implementations as the authors do have more data than us.\n",
    "\n",
    "2. ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output as we had done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
