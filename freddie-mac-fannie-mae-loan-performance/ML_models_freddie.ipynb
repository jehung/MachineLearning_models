{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# First iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get data and build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.tree as tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from joblib import Parallel, delayed\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets Used\n",
    "\n",
    "Freddie Mac has created a smaller dataset, which is a random sample of 50,000 loans selected from each full vintage year. Each vintage year has one origination data file and one monthly performance file, containing the same loan-level data fields as those included in the full dataset. We have located the `sample_2016.zip` file from the full dataset package, and used this zip package as our data source for this iteration.\n",
    "\n",
    "The 2016 zip packages has two files: `sample_orig_2016.txt` and `sample_svcg_2016.txt`. The .txt files do not come with headers but instead, we refer to the User Guide (http://www.freddiemac.com/research/pdf/user_guide.pdf) to grab the name of the columns. We then join the two data files together by the loan number. \n",
    "\n",
    "It is expected that as we progressed further, we will be using larger and larger datasets. But for this first iteration, this is what we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    dir = 'D:\\\\Backups\\\\StemData\\\\'\n",
    "    file = 'sample_orig_2016.txt'\n",
    "    file1 = 'sample_svcg_2016.txt'\n",
    "\n",
    "    raw = pd.read_csv(dir+file, sep='|', header=None)\n",
    "    raw.columns = ['credit_score', 'first_pmt_date', 'first_time', 'mat_date', 'msa', 'mi_perc', 'units',\n",
    "                    'occ_status', 'ocltv', 'odti', 'oupb', 'oltv', 'oint_rate', 'channel', 'ppm', 'fixed_rate',\n",
    "                    'state', 'prop_type','zip','loan_num', 'loan_purpose','oterm','num_borrowers', 'seller_name',\n",
    "                    'servicer_name','exceed_conform']\n",
    "\n",
    "    raw1 = pd.read_csv(dir+file1, sep='|', header=None)\n",
    "    raw1.columns = ['loan_num', 'yearmon', 'curr_upb','curr_delinq','loan_age','remain_months', 'repurchased',\n",
    "                     'modified', 'zero_bal','zero_date','curr_rate','curr_def_upb', 'ddlpi','mi_rec','net_proceeds',\n",
    "                     'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']\n",
    "\n",
    "    data = pd.merge(raw, raw1, on='loan_num', how='inner')\n",
    "\n",
    "    data.drop(['seller_name', 'servicer_name', 'first_pmt_date', 'mat_date', 'msa'], axis=1, inplace=True)\n",
    "    # all data must have the following: credit_score, ocltv, odti, oltv, oint_rate, curr_upb\n",
    "    # remove any datapoints with missing values from the above features\n",
    "    data.dropna(subset=['credit_score', 'odti', 'oltv', 'oint_rate', 'curr_upb'], how='any',inplace=True)\n",
    "    data.credit_score = pd.to_numeric(data['credit_score'], errors='coerce')\n",
    "    data.yearmon = pd.to_datetime(data['yearmon'], format='%Y%m')\n",
    "    data.fillna(value=0, inplace=True, axis=1)\n",
    "    \n",
    "    data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this into the next func\n",
    "    data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this into the next func\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment of Missing Values (So Far)\n",
    "\n",
    "Key features that are missing are more likely to be the result of reporting errors by the originator or the servicer, or incomplete information provided by the borrower. Similar to the Deep Learning paper we are reading, we have insisted that an observation must have no missing values in any of the following:\n",
    "\n",
    "* FICO score\n",
    "\n",
    "* LTV ratio\n",
    "\n",
    "* Original interest rate\n",
    "\n",
    "* original balance\n",
    "\n",
    "Samples missing one of these variables are removed. \n",
    "\n",
    "After this step, we still have lots of missing values -- a lot of them came from the performance file (such as loan modification costs, legal expenses, etc). Our treatment so far is to treat the missing values as zero, as an missing value of these fields tend to be due to the fact that there hasn't been such an incidence yet.\n",
    "\n",
    "It is clear that we will need to fine-tune our current treatment of missing values. This will be done in the second iteration by leveraging research already done by other STEM interns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = get_data()\n",
    "raw.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Space\n",
    "\n",
    "Here, we also model after the Deep Learning for Mortgage Risk paper. In the paper, the authors have enumerated the possible states (current, 30 days delinquent, etc), and together, with other loan_level features (listed in Table 2 and Table 6 in the paper), formed the feature space for their model.\n",
    "\n",
    "We do similar things here. The following code chunk further process the data: \n",
    "\n",
    "* Get the delinquency status that is associated with the loans and last observed month, and add a data column called `prev_delin`, in contrast with `curr_delinq`\n",
    "\n",
    "* Remove the `curr_delinq` from our features but the feature space still has `prev_delinq` variable\n",
    "\n",
    "* Use `curr_delinq` as our taget\n",
    "\n",
    "* For the categorical variables, we convert them into dummy/indicator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    #data.sort_values(['loan_num'], ascending=True).groupby(['yearmon'], as_index=False)  ##consider move this out\n",
    "    #data.set_index(['loan_num', 'yearmon'], inplace=True) ## consider move this out\n",
    "    y = data['curr_delinq']\n",
    "    y = y.apply(lambda x:1 if x not in (0, 1) else 0)\n",
    "    #data['prev_delinq'] = data.curr_delinq.shift(1) ## needs attention here\n",
    "    data['prev_delinq'] = data.groupby(level=0)['curr_delinq'].shift(1)\n",
    "    print(sum(data.prev_delinq.isnull()))\n",
    "    data.fillna(value=0, inplace=True, axis=1)\n",
    "    data.drop(['curr_delinq'], axis=1, inplace=True)\n",
    "    print(y.shape)\n",
    "    ## how many classes are y?\n",
    "    ## remove y from X\n",
    "    X = pd.get_dummies(data, columns=['first_time', 'occ_status', 'channel', 'ppm', 'fixed_rate',\n",
    "                                  'state', 'prop_type', 'loan_purpose', 'exceed_conform', 'repurchased'])\n",
    "    #y = label_binarize(y, classes=[0, 1, 2, 3]) ## do we really have to do this?\n",
    "    X[['credit_score','mi_perc','units','ocltv', 'odti', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec','net_proceeds',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']] = \\\n",
    "    scale(X[['credit_score','mi_perc','units','ocltv', 'odti', 'oupb', 'oltv', 'oint_rate','zip',\n",
    "       'curr_upb','loan_age','remain_months', 'curr_rate','curr_def_upb', 'ddlpi','mi_rec','net_proceeds',\n",
    "       'non_mi_rec', 'exp', 'legal_costs','maint_exp','tax_insur', 'misc_exp', 'loss','mod_exp']], with_mean=False)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24981\n",
      "(203642,)\n"
     ]
    }
   ],
   "source": [
    "train, target = process_data(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203642, 111)\n",
      "                         credit_score  mi_perc    units     ocltv      odti  \\\n",
      "loan_num     yearmon                                                          \n",
      "F116Q1000017 2016-02-01      16.29859      0.0  4.01181  4.847363  4.715427   \n",
      "             2016-03-01      16.29859      0.0  4.01181  4.847363  4.715427   \n",
      "             2016-04-01      16.29859      0.0  4.01181  4.847363  4.715427   \n",
      "             2016-05-01      16.29859      0.0  4.01181  4.847363  4.715427   \n",
      "             2016-06-01      16.29859      0.0  4.01181  4.847363  4.715427   \n",
      "\n",
      "                             oupb      oltv  oint_rate       zip  oterm  \\\n",
      "loan_num     yearmon                                                      \n",
      "F116Q1000017 2016-02-01  0.772413  4.825378   8.934837  1.444024    360   \n",
      "             2016-03-01  0.772413  4.825378   8.934837  1.444024    360   \n",
      "             2016-04-01  0.772413  4.825378   8.934837  1.444024    360   \n",
      "             2016-05-01  0.772413  4.825378   8.934837  1.444024    360   \n",
      "             2016-06-01  0.772413  4.825378   8.934837  1.444024    360   \n",
      "\n",
      "                         num_borrowers  curr_upb  loan_age  remain_months  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01              2  0.771472  0.000000       4.883126   \n",
      "             2016-03-01              2  0.762994  0.377898       4.869562   \n",
      "             2016-04-01              2  0.762994  0.755796       4.855997   \n",
      "             2016-05-01              2  0.762994  1.133694       4.842433   \n",
      "             2016-06-01              2  0.762994  1.511591       4.828869   \n",
      "\n",
      "                         modified  zero_bal  zero_date  curr_rate  \\\n",
      "loan_num     yearmon                                                \n",
      "F116Q1000017 2016-02-01       0.0       0.0        0.0   8.934837   \n",
      "             2016-03-01       0.0       0.0        0.0   8.934837   \n",
      "             2016-04-01       0.0       0.0        0.0   8.934837   \n",
      "             2016-05-01       0.0       0.0        0.0   8.934837   \n",
      "             2016-06-01       0.0       0.0        0.0   8.934837   \n",
      "\n",
      "                         curr_def_upb  ddlpi  mi_rec  net_proceeds  \\\n",
      "loan_num     yearmon                                                 \n",
      "F116Q1000017 2016-02-01           0.0    0.0     0.0           0.0   \n",
      "             2016-03-01           0.0    0.0     0.0           0.0   \n",
      "             2016-04-01           0.0    0.0     0.0           0.0   \n",
      "             2016-05-01           0.0    0.0     0.0           0.0   \n",
      "             2016-06-01           0.0    0.0     0.0           0.0   \n",
      "\n",
      "                         non_mi_rec  exp  legal_costs  maint_exp  tax_insur  \\\n",
      "loan_num     yearmon                                                          \n",
      "F116Q1000017 2016-02-01         0.0  0.0          0.0        0.0        0.0   \n",
      "             2016-03-01         0.0  0.0          0.0        0.0        0.0   \n",
      "             2016-04-01         0.0  0.0          0.0        0.0        0.0   \n",
      "             2016-05-01         0.0  0.0          0.0        0.0        0.0   \n",
      "             2016-06-01         0.0  0.0          0.0        0.0        0.0   \n",
      "\n",
      "                         misc_exp  loss  mod_exp  prev_delinq  first_time_0  \\\n",
      "loan_num     yearmon                                                          \n",
      "F116Q1000017 2016-02-01       0.0   0.0      0.0          0.0             0   \n",
      "             2016-03-01       0.0   0.0      0.0          0.0             0   \n",
      "             2016-04-01       0.0   0.0      0.0          0.0             0   \n",
      "             2016-05-01       0.0   0.0      0.0          0.0             0   \n",
      "             2016-06-01       0.0   0.0      0.0          0.0             0   \n",
      "\n",
      "                         first_time_N  first_time_Y  occ_status_I  \\\n",
      "loan_num     yearmon                                                \n",
      "F116Q1000017 2016-02-01             0             1             0   \n",
      "             2016-03-01             0             1             0   \n",
      "             2016-04-01             0             1             0   \n",
      "             2016-05-01             0             1             0   \n",
      "             2016-06-01             0             1             0   \n",
      "\n",
      "                         occ_status_O  occ_status_S  channel_B  channel_C  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01             1             0          0          0   \n",
      "             2016-03-01             1             0          0          0   \n",
      "             2016-04-01             1             0          0          0   \n",
      "             2016-05-01             1             0          0          0   \n",
      "             2016-06-01             1             0          0          0   \n",
      "\n",
      "                         channel_R  ppm_0  ppm_N  fixed_rate_FRM  state_AK  \\\n",
      "loan_num     yearmon                                                         \n",
      "F116Q1000017 2016-02-01          1      0      1               1         0   \n",
      "             2016-03-01          1      0      1               1         0   \n",
      "             2016-04-01          1      0      1               1         0   \n",
      "             2016-05-01          1      0      1               1         0   \n",
      "             2016-06-01          1      0      1               1         0   \n",
      "\n",
      "                         state_AL  state_AR  state_AZ  state_CA  state_CO  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_CT  state_DC  state_DE  state_FL  state_GA  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_GU  state_HI  state_IA  state_ID  state_IL  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_IN  state_KS  state_KY  state_LA  state_MA  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_MD  state_ME  state_MI  state_MN  state_MO  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_MS  state_MT  state_NC  state_ND  state_NE  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_NH  state_NJ  state_NM  state_NV  state_NY  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_OH  state_OK  state_OR  state_PA  state_PR  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         1         0         0         0         0   \n",
      "             2016-03-01         1         0         0         0         0   \n",
      "             2016-04-01         1         0         0         0         0   \n",
      "             2016-05-01         1         0         0         0         0   \n",
      "             2016-06-01         1         0         0         0         0   \n",
      "\n",
      "                         state_RI  state_SC  state_SD  state_TN  state_TX  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_UT  state_VA  state_VI  state_VT  state_WA  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01         0         0         0         0         0   \n",
      "             2016-03-01         0         0         0         0         0   \n",
      "             2016-04-01         0         0         0         0         0   \n",
      "             2016-05-01         0         0         0         0         0   \n",
      "             2016-06-01         0         0         0         0         0   \n",
      "\n",
      "                         state_WI  state_WV  state_WY  prop_type_CO  \\\n",
      "loan_num     yearmon                                                  \n",
      "F116Q1000017 2016-02-01         0         0         0             0   \n",
      "             2016-03-01         0         0         0             0   \n",
      "             2016-04-01         0         0         0             0   \n",
      "             2016-05-01         0         0         0             0   \n",
      "             2016-06-01         0         0         0             0   \n",
      "\n",
      "                         prop_type_CP  prop_type_LH  prop_type_MH  \\\n",
      "loan_num     yearmon                                                \n",
      "F116Q1000017 2016-02-01             0             0             0   \n",
      "             2016-03-01             0             0             0   \n",
      "             2016-04-01             0             0             0   \n",
      "             2016-05-01             0             0             0   \n",
      "             2016-06-01             0             0             0   \n",
      "\n",
      "                         prop_type_PU  prop_type_SF  loan_purpose_C  \\\n",
      "loan_num     yearmon                                                  \n",
      "F116Q1000017 2016-02-01             0             1               0   \n",
      "             2016-03-01             0             1               0   \n",
      "             2016-04-01             0             1               0   \n",
      "             2016-05-01             0             1               0   \n",
      "             2016-06-01             0             1               0   \n",
      "\n",
      "                         loan_purpose_N  loan_purpose_P  exceed_conform_0  \\\n",
      "loan_num     yearmon                                                        \n",
      "F116Q1000017 2016-02-01               0               1                 1   \n",
      "             2016-03-01               0               1                 1   \n",
      "             2016-04-01               0               1                 1   \n",
      "             2016-05-01               0               1                 1   \n",
      "             2016-06-01               0               1                 1   \n",
      "\n",
      "                         exceed_conform_Y  repurchased_0  repurchased_N  \\\n",
      "loan_num     yearmon                                                      \n",
      "F116Q1000017 2016-02-01                 0              1              0   \n",
      "             2016-03-01                 0              1              0   \n",
      "             2016-04-01                 0              1              0   \n",
      "             2016-05-01                 0              1              0   \n",
      "             2016-06-01                 0              1              0   \n",
      "\n",
      "                         repurchased_Y  \n",
      "loan_num     yearmon                    \n",
      "F116Q1000017 2016-02-01              0  \n",
      "             2016-03-01              0  \n",
      "             2016-04-01              0  \n",
      "             2016-05-01              0  \n",
      "             2016-06-01              0  \n",
      "(203642,)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(train.head())\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting plotting utility ready\n",
    "\n",
    "We define the function to plot confusion matrix beow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree with Pruning\n",
    "\n",
    "Decision Trees. For the decision tree, you should implement or steal a decision tree algorithm (and by \"implement or steal\" I mean \"steal\"). Be sure to use some form of pruning. You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw.prev_delinq.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complexity_dt(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=778)\n",
    "    smote = SMOTE(ratio=1)\n",
    "    X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    clf = tree.DecisionTreeClassifier(criterion='gini', class_weight='balanced')\n",
    "    pipe = Pipeline([('smote', smote), ('dt', clf)])\n",
    "    param_grid = {'dt__max_depth': [2, 3, 4, 5, 6, 7, 8]}\n",
    "    #sss = StratifiedShuffleSplit(n_splits=500, test_size=0.2)  ## no need for this given 50000 random sample\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, n_jobs=6, cv=10, scoring='neg_log_loss',verbose=5)\n",
    "    grid_search.fit(X_train_res, y_train_res)\n",
    "    clf = grid_search.best_estimator_\n",
    "    print('clf', clf)\n",
    "    print('best_score', grid_search.best_score_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    check_pred = clf.predict(X_train)\n",
    "    target_names = ['curr_delinq 0', 'curr_delinq 1', 'curr_delinq 2', 'curr_delinq 3']\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=target_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "    plt.show()    \n",
    "    return clf, clf.predict(X_train_res), y_pred\n",
    "\n",
    "\n",
    "dt, predict_dt, result_dt = complexity_dt(train, target)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.itemfreq(predict_dt)\n",
    "scipy.stats.itemfreq(result_dt)\n",
    "\n",
    "\n",
    "#import graphviz \n",
    "#dot_data = tree.export_graphviz(dt, out_file=None) \n",
    "#graph = graphviz.Source(dot_data) \n",
    "#graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def traing_size_dt(X, y):\n",
    "    d = {'train':[], 'test':[]}\n",
    "    for size in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=size, random_state=778)\n",
    "        #smote = SMOTE(ratio=1)\n",
    "        #X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "    \n",
    "   \n",
    "        #X_train_sub = X_train_res[np.random.choice(a=int(X_train_res.shape[0]*size), size=int(X_train_res.shape[0]*size), replace=False), :]\n",
    "        #y_train_sub = y_train_res[np.random.choice(a=int(X_train_res.shape[0]*size), size=int(X_train_res.shape[0]*size), replace=False)]\n",
    "        #print(X_train_res.shape)\n",
    "        #print(X_train_sub.shape)\n",
    "        \n",
    "        clf = tree.DecisionTreeClassifier(criterion='gini', class_weight='balanced')\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        check_pred = clf.predict(X_train)\n",
    "        from collections import defaultdict\n",
    "        from sklearn.metrics import f1_score\n",
    "        \n",
    "        #d['size '+str((size*10))].append(f1_score(y_train, check_pred, average='weighted')) \n",
    "        #d['size '+str((size*10))].append(f1_score(y_test, y_pred, average='weighted')) \n",
    "        d['train'].append(f1_score(y_train, check_pred, average='weighted'))\n",
    "        d['test'].append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        \n",
    "    return d\n",
    "    \n",
    "\n",
    "    #return clf, clf.predict(X_test), clf.predict(X_train_res)\n",
    "\n",
    "\n",
    "dt = traing_size_dt(train, target)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_res, y_train_res = smote.fit_sample(X_train, y_train)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X_train_res, y, train_size=size, random_state=778)\n",
    "        clf = tree.DecisionTreeClassifier(criterion='gini', class_weight='balanced', max_depth=8)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        check_pred = clf.predict(X_train_res)\n",
    "        from collections import defaultdict\n",
    "        from sklearn.metrics import f1_score\n",
    "        \n",
    "        d = defaultdict(list)\n",
    "        d['size '+str((size*10))].append(f1_score(y_train_sub, check_pred, average='weighted')) \n",
    "        d['size '+str((size*10))].append(f1_score(y_test, y_pred, average='weighted')) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.itemfreq(check)\n",
    "scipy.stats.itemfreq(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "test\n",
      "[0.99611869861163338, 0.9958699281155543, 0.99608532855047971, 0.9964696683606088, 0.99644642539544015, 0.99641175357879663, 0.99663268022734319, 0.99638024459529995, 0.99724173027991192]\n"
     ]
    }
   ],
   "source": [
    "for e in dt:\n",
    "    print(e)\n",
    "    print(dt[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Network Model: First Iteration\n",
    "\n",
    "\n",
    "We had used the grid search approach to find the the best number of hidden layers (out of 1, 3, 5, and 7). For each of these options, we started out with the full set of features, then reduce it to 70% of that for each subsequent hidden layers.\n",
    "\n",
    "The authors' deep learning give them a probability transition matrix. \n",
    "\n",
    "Our model below gives us a probability matrix for each observation data. This is slightly different.\n",
    "\n",
    "However, with a bit more work, we can convert our probability matrix produced from our model into the probability transition matrix, so that it not only predicts for us, when a new data comes in, what is the most likely delinquent status of a new loan, but also tell us what is the probability that a loan of a particular delinquency status will transition into a different status type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridSearch_nn(X, y):\n",
    "    #X_train, y_train, X_test, y_test = train_test_split(X,y)\n",
    "    mlp = MLPClassifier(solver='adam', alpha=1e-5, shuffle=True, learning_rate='invscaling',\n",
    "         verbose=True)\n",
    "    parameters = {'hidden_layer_sizes':[(519), (519, 363, 363), (519, 363, 363, 363, 363), (519, 363, 363, 363, 363, 363, 363)]}\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)  ## no need for this given 50000 random sample\n",
    "    gs = GridSearchCV(estimator=mlp, param_grid=parameters, n_jobs=6, cv=sss, scoring='roc_auc',verbose=5)\n",
    "    gs.fit(X, y)\n",
    "    clf = gs.best_estimator_\n",
    "    print(clf)\n",
    "    print(gs.best_score_)\n",
    "    mat = clf.predict_proba(X)\n",
    "    print(mat)\n",
    "    \n",
    "    return clf, gs.best_score_, mat\n",
    "\n",
    "\n",
    "if  __name__== '__main__':\n",
    "    clf, score, mat = gridSearch_nn(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notes \n",
    "\n",
    "1. We had 519 variables and the authors had 294. But to be sure, we don't have a greater number of features compared to the authors. I think this is just an artifact of our different implementations as the authors do have more data than us.\n",
    "\n",
    "2. ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output as we had done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
